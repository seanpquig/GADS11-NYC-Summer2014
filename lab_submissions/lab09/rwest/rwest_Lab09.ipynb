{
 "metadata": {
  "name": "",
  "signature": "sha256:ed670c86e5d43df79d65381057d0bb3824dabd41a6795839c4dc728971c268b4"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# If you aren't running this from `lab_submissions/lab09/$FLASTNAME` then copy it over and start again."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Probability\n",
      "\n",
      "We're in search of the perfect soft-boiled egg. The eggs are from different sources, and we don't have a timer. So the brand of the egg and the length of time they boil are random."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Joint Probability\n",
      "\n",
      "In words, what does $P(brand=Acme, time=optimal)$ refer to?\n",
      "<font color='blue'>\n",
      "What is the probability that the brand is Acme and that the time is optimal?\n",
      "</font>\n",
      "\n",
      "Is it the same as $P(brand=Acme)$ or $P(time=optimal)$?\n",
      "<font color='blue'>\n",
      "No\n",
      "</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conditional Probability\n",
      "\n",
      "In words, what does $P(time=optimal | brand=Acme)$ refer to?\n",
      "\n",
      "<font color='blue'>\n",
      "What is the probability that the time is optimal given that the brand is Acme? \n",
      "</font>\n",
      "\n",
      "Is it the same as $P(brand=Acme | time=optimal)$?  yes/no\n",
      "\n",
      "<font color='blue'>\n",
      "Nope.\n",
      "</font>    \n",
      "\n",
      "Is it the same as $P(brand=Acme, time=optimal)$?  yes/no\n",
      "\n",
      "<font color='blue'>\n",
      "Nope.\n",
      "</font>\n",
      "\n",
      "What is the relationship between the conditional and the joint probabilities?\n",
      "\n",
      "<font color='blue'>\n",
      "First let's define the following:\n",
      "\n",
      "$$ \\text{Event A} \\triangleq  (\\text{time is optimal}) $$\n",
      "$$\\text{Event B} \\triangleq (\\text{brand is Acme})  $$\n",
      "\n",
      "We can observe this from the definition of conditional probability:\n",
      "\n",
      "$$P(A | B) = \\frac{P(A, B)}{P(B)}, P(B)>0$$\n",
      "</font>   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Independence\n",
      "\n",
      "It's not unreasonable to think the optimal boil time has nothing to do with the brand of the egg. In this case, how do the conditional probabilities change?\n",
      "\n",
      "<font color='blue'>\n",
      "From this statement we could assume that the events are Independent, this implies that:\n",
      "    \n",
      "$$P(A, B) = P(A)P(B)$$\n",
      "</font>\n",
      "\n",
      "What is now the expression for the joint probability in terms of the new, simplified joint probabilities?\n",
      "\n",
      "<font color='blue'>\n",
      "which then intern implies that:\n",
      "    \n",
      "$$P(A | B) = \\frac{P(A)P(B)}{P(B)} = P(A)$$\n",
      "</font>   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bayes's Theorem\n",
      "\n",
      "Let's assume the different egg brands actually do have different ideal boil times. i.e. let's drop the independence assumption.\n",
      "\n",
      "We've boiled 100 eggs now. When we ate one that was cooked ideally, we took the pains to test the brand of the egg and found that of the 10 ideal eggs, 5 were Acme. We also know that we have 5 brands in our basket.\n",
      "\n",
      "Since Acme eggs seemed to perform well, we bought a bunch of Acme eggs. Given that we know we're about to crack open an Acme egg, what is a reasonable estimation of the probability that the egg is boiled ideally? What is $P(time=optimal | brand=Acme)$?\n",
      "\n",
      "<font color='blue'> \n",
      "Baye's Theorem gives us the following (which comes from the def'n of conditional probability and the fact that $P(A,B)=P(B,A)$:\n",
      "\n",
      "\n",
      "$$P(A | B) = \\frac{P(B | A)P(A)}{P(B)}$$\n",
      "\n",
      "We know that $ P(B) = 1 $ since we know we have an acme egg. We can estimate $P(B | A)$ from the empirical evidence that of the 10 ideal eggs, 5 were acme. i.e. $P(B | A) = \\frac{5}{10} = \\frac{1}{2}$, further more we can estimate the probability that the egg will be boiled opimally from the 10 out of 100 observed, $P(A)=\\frac{1}{10}$, which gives:\n",
      "\n",
      "$$P(A | B) = \\frac{1/2 \\cdot 1/10}{1} = \\frac{1}{20}$$\n",
      "\n",
      "5 brands? is that a red herring?\n",
      "\n",
      "</font>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Naive Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Optional: Proof\n",
      "\n",
      "Starting with the equation in the slides, prove that the independence assumption leads directly to the final form. i.e., show that $P(x_1, x_2, ..., x_n | C)$ leads to $P(x_1 | C) \\cdot P(x_2 | C) \\cdot P(x_3 | C) \\cdot \\dots P(x_n | C)$\n",
      "\n",
      "Show work here:\n",
      "\n",
      "...\n",
      "\n",
      "..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multinomial vs. Bernoulli NB:\n",
      "![](http://note.io/1ftU7X6)\n",
      "\n",
      "**Multinomial Model** actually counts occurences out of all possible occurences for probability - better for greater features\n",
      "\n",
      "The class equals:\n",
      "$$\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
      "Class = \\argmax_C P(Beijing|C) \\cdot P(and|C) \\cdot P(Taipei|C) \\cdot P(join|C) \\cdot P(WTO|C)$$\n",
      "over all classes C.\n",
      "\n",
      "**Bernoulli model** counts only all documents with presence of the word - better for fewer features\n",
      "\n",
      "The class equals:\n",
      "$$\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
      "Class = \\argmax_C P(Alaska=0|C) \\cdot P(Beijing=1|C) \\cdot P(India=0|C) \\cdot P(join=1|C) \\cdot P(Taipei=1|C) \\cdot P(WTO=1|C)$$\n",
      "over all classes C."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB, BernoulliNB  # Naive Bayes model classes\n",
      "from sklearn.feature_extraction.text import CountVectorizer  # convert text into feature vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "### How the Count Vectorizer Works -- Just run and read for the next few cells\n",
      "#\n",
      "\n",
      "text = ['Math is great', 'Math is really great', 'Exciting exciting Math']\n",
      "print \"Original text is:\\n\", '\\n'.join(text)\n",
      "\n",
      "vectorizer = CountVectorizer()\n",
      "\n",
      "# call `fit` to build the vocabulary\n",
      "vectorizer.fit(text)\n",
      "print\n",
      "print 'features'\n",
      "print list(enumerate(vectorizer.get_feature_names()))\n",
      "\n",
      "# call `transform` to convert text to a bag of words\n",
      "x = vectorizer.transform(text)\n",
      "print\n",
      "print '(document, feature)  count'\n",
      "print x\n",
      "print\n",
      "print 'the same data in matrix form:'\n",
      "print x.toarray()\n",
      "\n",
      "# Notice that the bag of words treatment doesn't preserve information about the *order* of words, \n",
      "# just their frequency"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Original text is:\n",
        "Math is great\n",
        "Math is really great\n",
        "Exciting exciting Math\n",
        "\n",
        "features\n",
        "[(0, u'exciting'), (1, u'great'), (2, u'is'), (3, u'math'), (4, u'really')]\n",
        "\n",
        "(document, feature)  count\n",
        "  (0, 1)\t1\n",
        "  (0, 2)\t1\n",
        "  (0, 3)\t1\n",
        "  (1, 1)\t1\n",
        "  (1, 2)\t1\n",
        "  (1, 3)\t1\n",
        "  (1, 4)\t1\n",
        "  (2, 0)\t2\n",
        "  (2, 3)\t1\n",
        "\n",
        "the same data in matrix form:\n",
        "[[0 1 1 1 0]\n",
        " [0 1 1 1 1]\n",
        " [2 0 0 1 0]]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You can include bigrams and larger in your model as well\n",
      "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
      "\n",
      "vectorizer.fit(text)\n",
      "print\n",
      "print 'features'\n",
      "print list(enumerate(vectorizer.get_feature_names()))\n",
      "\n",
      "x = vectorizer.transform(text)\n",
      "print\n",
      "print '(document, feature)  count'\n",
      "print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "features\n",
        "[(0, u'exciting'), (1, u'exciting exciting'), (2, u'exciting math'), (3, u'great'), (4, u'is'), (5, u'is great'), (6, u'is really'), (7, u'math'), (8, u'math is'), (9, u'really'), (10, u'really great')]\n",
        "\n",
        "(document, feature)  count\n",
        "  (0, 3)\t1\n",
        "  (0, 4)\t1\n",
        "  (0, 5)\t1\n",
        "  (0, 7)\t1\n",
        "  (0, 8)\t1\n",
        "  (1, 3)\t1\n",
        "  (1, 4)\t1\n",
        "  (1, 6)\t1\n",
        "  (1, 7)\t1\n",
        "  (1, 8)\t1\n",
        "  (1, 9)\t1\n",
        "  (1, 10)\t1\n",
        "  (2, 0)\t2\n",
        "  (2, 1)\t1\n",
        "  (2, 2)\t1\n",
        "  (2, 7)\t1\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Wikipedia\n",
      "\n",
      "## Now let's fetch some real text samples.\n",
      "\n",
      "Little known fact: The greatest value of Wikipedia lies not in the incredibly broad collection of human knowledge, but in providing Machine Learning researchers with an easily accessible corpus with which to train their models.\n",
      "\n",
      "Just kidding. Sort of.\n",
      "\n",
      "### In your terminal, type `pip install wikipedia`. If that doesn't work, `sudo pip install wikipedia`.\n",
      "\n",
      "We're going to fetch a bunch of documents from the English and Simple English versions of Wikipedia. We'll train our classifier on these documents and then see if it can guess whether new documents came from one or the other.\n",
      "\n",
      "The fun begins...."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import wikipedia as wiki  # wikipedia api wrapper\n",
      "wiki.set_rate_limiting(True)  # might actually speed things up.\n",
      "from sklearn.cross_validation import train_test_split  # split the data you have into training and test sets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fetch_wiki(title, lang):\n",
      "    '''\n",
      "    Return the regular English or simple versions of an article.\n",
      "    Simple versions are far shorter than the regular ones, so only\n",
      "    pull the summary of regular articles.\n",
      "    In case of an error, just return None instead of crashing the program.\n",
      "    '''\n",
      "    assert lang in ('en', 'simple'), \"Language must be 'en' or 'simple'\"\n",
      "\n",
      "    try:\n",
      "        wiki.set_lang(lang)\n",
      "        page = wiki.page(title)\n",
      "        # print page.title  # used for testing the function\n",
      "        return (page.summary, 1) if lang == 'en' else (page.content, 0)  # 1: english, 0: simple\n",
      "    except:  # NOTE: you should never have a blind `except` like this. but, hey, we're hacking.\n",
      "        print ' - error with ' + lang + ' page for: ' + title\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles = ['General Relativity', 'Bayes Theorem', 'Ada Lovelace',\n",
      "            'jackfruit', 'mantis shrimp', 'Der Ring des Nibelungen',\n",
      "            'antikythera mechanism', 'teflon', 'superconductor',\n",
      "            'Harper Lee', 'durian', 'Shostakovich', 'steel',\n",
      "            'database', 'transistor', 'Goethe', 'dog', 'meme', 'spleen',\n",
      "            'morphine', 'maple', \"rubik's cube\", 'souffle', 'chlorine',\n",
      "            'earthworm', 'prune', 'ballet', 'ultrasound', 'bruce lee']\n",
      "\n",
      "corpus = []\n",
      "\n",
      "# search for each article, and if we get a result, store it in the corpus\n",
      "for article in articles:\n",
      "    en = fetch_wiki(article, 'en')\n",
      "    if en:\n",
      "        corpus.append(en)\n",
      "    simple = fetch_wiki(article, 'simple')\n",
      "    if simple:\n",
      "        corpus.append(simple)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert features\n",
      "text, Y = zip(*corpus)\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use SKLearn's train_test_split \n",
      "xtrain, xtest, ytrain, ytest = train_test_split(X, Y)\n",
      "\n",
      "# Create our classifier\n",
      "clf = MultinomialNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n",
      "\n",
      "clf = BernoulliNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 35.48%\n",
        "Accuracy: 54.84%\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# More data\n",
      "\n",
      "That didn't work so well. You can repeat the last cell a few times to see much the accuracy depends on the train/test split. This calls for more data.\n",
      "\n",
      "Use the `.random(n_titles)` method to retrieve a list of 100 more titles and fetch the text for training. Note that `.random` will only return up to 10 titles at a time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wiki.set_lang('simple')\n",
      "\n",
      "for i in xrange(10):\n",
      "    print i  # monitor progress -- this is really slow\n",
      "    for title in wiki.random(10):\n",
      "        en = fetch_wiki(title, 'en')\n",
      "        if en:\n",
      "            corpus.append(en)\n",
      "        simple = fetch_wiki(title, 'simple')\n",
      "        if simple:\n",
      "            corpus.append(simple)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " - error with en page for: Snow White (disambiguation)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " - error with simple page for: Snow White (disambiguation)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " - error with en page for: Rize"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "3"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "4"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " - error with en page for: Gold rush (disambiguation)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " - error with simple page for: Gold rush (disambiguation)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " - error with en page for: Andromeda"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " - error with simple page for: Andromeda"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "5"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "6"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "7"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " - error with en page for: Tengiz"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " - error with en page for: George Graham"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "8"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "9"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " - error with en page for: Shiloh"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we just spent a long time fetching this data, so let's save it so we don't have to do it all over again.\n",
      "import pickle\n",
      "pickle.dump(corpus, open('corpus.p', 'wb'))\n",
      "\n",
      "# restore with: corpus = pickle.load(open('corpus.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert features\n",
      "text, Y = zip(*corpus)\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(text)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use SKLearn's train_test_split \n",
      "import numpy as np\n",
      "\n",
      "multi_accuracy = []\n",
      "bernoulli_accuracy = []\n",
      "num_trials = 100\n",
      "\n",
      "for i in xrange(num_trials):\n",
      "    xtrain, xtest, ytrain, ytest = train_test_split(X, Y)\n",
      "\n",
      "    # Create our classifier\n",
      "    clf = MultinomialNB().fit(xtrain, ytrain)\n",
      "    \n",
      "    multi_accuracy.append(100 * clf.score(xtest, ytest))\n",
      "    #print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n",
      "\n",
      "    clf = BernoulliNB().fit(xtrain, ytrain)\n",
      "\n",
      "    bernoulli_accuracy.append(100 * clf.score(xtest, ytest))\n",
      "    #print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n",
      "    \n",
      "print \"Multi Accuracy Mean: %0.2f%%\" % np.mean(multi_accuracy)\n",
      "print \"Multi Accuracy StdErr: %0.2f%%\" % (np.std(multi_accuracy)/np.sqrt(num_trials))\n",
      "print \n",
      "print \"bernoulli Accuracy Mean: %0.2f%%\" % np.mean(bernoulli_accuracy)\n",
      "print \"bernoulli Accuracy StdErr: %0.2f%%\" % (np.std(bernoulli_accuracy)/np.sqrt(num_trials))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Multi Accuracy Mean: 35.89%\n",
        "Multi Accuracy StdErr: 0.60%\n",
        "\n",
        "bernoulli Accuracy Mean: 48.73%\n",
        "bernoulli Accuracy StdErr: 0.58%\n"
       ]
      }
     ],
     "prompt_number": 170
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The variance is a bit contained, but it's still not performing too well. Now it's your turn to try a few things to see if you can improve the performance to something useful. You can try fetching even more data overnight. We'd only fetched the English article summaries, but maybe it'll help to use the full articles. You can try filtering out words < 3 or 4 chars long. We haven't tried allowing bigrams and trigrams yet or removed stop-words. Both the Multinomial and Bernoulli models have model parameters that you can try adjusting. Finally, you might imagine other ways to tweak our model!\n",
      "\n",
      "Here are links to relevant docs:\n",
      "\n",
      "[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
      "[MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
      "[BernoulliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# lets get some stopwords NLTK (wow last's project is already useful!)\n",
      "\n",
      "from nltk.corpus import stopwords\n",
      "std_word_set=set(stopwords.words('english'))\n",
      "\n",
      "# put text in a list, instead of a tuple so we can edit it   \n",
      "new_text = []\n",
      "for x in text:\n",
      "    new_text.append(x)\n",
      "    \n",
      "# remove stopwords from text\n",
      "for stopword in std_word_set:\n",
      "    new_text = [x.replace(\" \" + stopword + \" \",\" \") for x in new_text]\n",
      "\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(new_text)\n",
      "\n",
      "# Use SKLearn's train_test_split \n",
      "xtrain, xtest, ytrain, ytest = train_test_split(X, Y)\n",
      "\n",
      "# Create our classifier\n",
      "clf = MultinomialNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n",
      "\n",
      "clf = BernoulliNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 35.48%\n",
        "Accuracy: 53.23%\n"
       ]
      }
     ],
     "prompt_number": 173
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}