{
 "metadata": {
  "name": "",
  "signature": "sha256:ab010cd12f455fce157c259a881c9ca163f6e9b97aa40e82cf9ce7e75b1fce3c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# If you aren't running this from `lab_submissions/lab08/$FLASTNAME` then copy it over and start again."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Probability\n",
      "\n",
      "We're in search of the perfect soft-boiled egg. The eggs are from different sources, and we don't have a timer. So the brand of the egg and the length of time they boil are random."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Joint Probability\n",
      "\n",
      "In words, what does $P(brand=Acme, time=optimal)$ refer to?\n",
      "\n",
      "Is it the same as $P(brand=Acme)$ or $P(time=optimal)$?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Conditional Probability\n",
      "\n",
      "In words, what does $P(time=optimal | brand=Acme)$ refer to?\n",
      "\n",
      "Is it the same as $P(brand=Acme | time=optimal)$?  yes/no\n",
      "\n",
      "Is it the same as $P(brand=Acme, time=optimal)$?  yes/no\n",
      "\n",
      "What is the relationship between the conditional and the joint probabilities?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Independence\n",
      "\n",
      "It's not unreasonable to think the optimal boil time has nothing to do with the brand of the egg. In this case, how do the conditional probabilities change?\n",
      "\n",
      "What is now the expression for the joint probability in terms of the new, simplified joint probabilities?\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Bayes's Theorem\n",
      "\n",
      "Let's assume the different egg brands actually do have different ideal boil times. i.e. let's drop the independence assumption.\n",
      "\n",
      "We've boiled 100 eggs now. When we ate one that was cooked ideally, we took the pains to test the brand of the egg and found that of the 10 ideal eggs, 5 were Acme. We also know that we have 5 brands in our basket.\n",
      "\n",
      "Since Acme eggs seemed to perform well, we bought a bunch of Acme eggs. Given that we know we're about to crack open an Acme egg, what is a reasonable estimation of the probability that the egg is boiled ideally? What is $P(time=optimal | brand=Acme)$?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Naive Bayes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Optional: Proof\n",
      "\n",
      "Starting with the equation in the slides, prove that the independence assumption leads directly to the final form. i.e., show that $P(x_1, x_2, ..., x_n | C)$ leads to $P(x_1 | C) \\cdot P(x_2 | C) \\cdot P(x_3 | C) \\cdot \\dots P(x_n | C)$\n",
      "\n",
      "Show work here:\n",
      "\n",
      "...\n",
      "\n",
      "..."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Multinomial vs. Bernoulli NB:\n",
      "![](http://note.io/1ftU7X6)\n",
      "\n",
      "**Multinomial Model** actually counts occurences out of all possible occurences for probability - better for greater features\n",
      "\n",
      "The class equals:\n",
      "$$\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
      "Class = \\argmax_C P(Beijing|C) \\cdot P(and|C) \\cdot P(Taipei|C) \\cdot P(join|C) \\cdot P(WTO|C)$$\n",
      "over all classes C.\n",
      "\n",
      "**Bernoulli model** counts only all documents with presence of the word - better for fewer features\n",
      "\n",
      "The class equals:\n",
      "$$\\DeclareMathOperator*{\\argmax}{\\arg\\!\\max}\n",
      "Class = \\argmax_C P(Alaska=0|C) \\cdot P(Beijing=1|C) \\cdot P(India=0|C) \\cdot P(join=1|C) \\cdot P(Taipei=1|C) \\cdot P(WTO=1|C)$$\n",
      "over all classes C."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB, BernoulliNB  # Naive Bayes model classes\n",
      "from sklearn.feature_extraction.text import CountVectorizer  # convert text into feature vectors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print vectorizer.transform(text).toarray()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#\n",
      "### How the Count Vectorizer Works -- Just run and read for the next few cells\n",
      "#\n",
      "\n",
      "text = ['Math is great', 'Math is really great', 'Exciting exciting Math']\n",
      "print \"Original text is:\\n\", '\\n'.join(text)\n",
      "\n",
      "vectorizer = CountVectorizer()\n",
      "\n",
      "# call `fit` to build the vocabulary\n",
      "vectorizer.fit(text)\n",
      "print\n",
      "print 'features'\n",
      "print list(enumerate(vectorizer.get_feature_names()))\n",
      "\n",
      "# call `transform` to convert text to a bag of words\n",
      "x = vectorizer.transform(text)\n",
      "print\n",
      "print '(document, feature)  count'\n",
      "print x\n",
      "print\n",
      "print 'the same data in matrix form:'\n",
      "print x.toarray()\n",
      "\n",
      "# Notice that the bag of words treatment doesn't preserve information about the *order* of words, \n",
      "# just their frequency"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Original text is:\n",
        "Math is great\n",
        "Math is really great\n",
        "Exciting exciting Math\n",
        "\n",
        "features\n",
        "[(0, u'exciting'), (1, u'great'), (2, u'is'), (3, u'math'), (4, u'really')]\n",
        "\n",
        "(document, feature)  count\n",
        "  (0, 1)\t1\n",
        "  (0, 2)\t1\n",
        "  (0, 3)\t1\n",
        "  (1, 1)\t1\n",
        "  (1, 2)\t1\n",
        "  (1, 3)\t1\n",
        "  (1, 4)\t1\n",
        "  (2, 0)\t2\n",
        "  (2, 3)\t1\n",
        "\n",
        "the same data in matrix form:\n",
        "[[0 1 1 1 0]\n",
        " [0 1 1 1 1]\n",
        " [2 0 0 1 0]]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# You can include bigrams and larger in your model as well\n",
      "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
      "\n",
      "vectorizer.fit(text)\n",
      "print\n",
      "print 'features'\n",
      "print list(enumerate(vectorizer.get_feature_names()))\n",
      "\n",
      "x = vectorizer.transform(text)\n",
      "print\n",
      "print '(document, feature)  count'\n",
      "print x\n",
      "print\n",
      "print 'the same data in matrix form:'\n",
      "print x.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "features\n",
        "[(0, u'exciting'), (1, u'exciting exciting'), (2, u'exciting math'), (3, u'great'), (4, u'is'), (5, u'is great'), (6, u'is really'), (7, u'math'), (8, u'math is'), (9, u'really'), (10, u'really great')]\n",
        "\n",
        "(document, feature)  count\n",
        "  (0, 3)\t1\n",
        "  (0, 4)\t1\n",
        "  (0, 5)\t1\n",
        "  (0, 7)\t1\n",
        "  (0, 8)\t1\n",
        "  (1, 3)\t1\n",
        "  (1, 4)\t1\n",
        "  (1, 6)\t1\n",
        "  (1, 7)\t1\n",
        "  (1, 8)\t1\n",
        "  (1, 9)\t1\n",
        "  (1, 10)\t1\n",
        "  (2, 0)\t2\n",
        "  (2, 1)\t1\n",
        "  (2, 2)\t1\n",
        "  (2, 7)\t1\n",
        "\n",
        "the same data in matrix form:\n",
        "[[0 0 0 1 1 1 0 1 1 0 0]\n",
        " [0 0 0 1 1 0 1 1 1 1 1]\n",
        " [2 1 1 0 0 0 0 1 0 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Wikipedia\n",
      "\n",
      "## Now let's fetch some real text samples.\n",
      "\n",
      "Little known fact: The greatest value of Wikipedia lies not in the incredibly broad collection of human knowledge, but in providing Machine Learning researchers with an easily accessible corpus with which to train their models.\n",
      "\n",
      "Just kidding. Sort of.\n",
      "\n",
      "### In your terminal, type `pip install wikipedia`. If that doesn't work, `sudo pip install wikipedia`.\n",
      "\n",
      "We're going to fetch a bunch of documents from the English and Simple English versions of Wikipedia. We'll train our classifier on these documents and then see if it can guess whether new documents came from one or the other.\n",
      "\n",
      "The fun begins...."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import wikipedia as wiki  # wikipedia api wrapper\n",
      "wiki.set_rate_limiting(True)  # might actually speed things up.\n",
      "from sklearn.cross_validation import train_test_split  # split the data you have into training and test sets\n",
      "print 4"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fetch_wiki(title, lang):\n",
      "    '''\n",
      "    Return the regular English or simple versions of an article.\n",
      "    Simple versions are far shorter than the regular ones, so only\n",
      "    pull the summary of regular articles.\n",
      "    In case of an error, just return None instead of crashing the program.\n",
      "    '''\n",
      "    assert lang in ('en', 'simple'), \"Language must be 'en' or 'simple'\"\n",
      "\n",
      "    try:\n",
      "        wiki.set_lang(lang)\n",
      "        page = wiki.page(title)\n",
      "        # print page.title  # used for testing the function\n",
      "        return (page.summary, 1) if lang == 'en' else (page.content, 0)  # 1: english, 0: simple\n",
      "    except:  # NOTE: you should never have a blind `except` like this. but, hey, we're hacking.\n",
      "        print ' - error with ' + lang + ' page for: ' + title\n",
      "        return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles = ['General Relativity', 'Bayes Theorem', 'Ada Lovelace',\n",
      "            'jackfruit', 'mantis shrimp', 'Der Ring des Nibelungen',\n",
      "            'antikythera mechanism', 'teflon', 'superconductor',\n",
      "            'Harper Lee', 'durian', 'Shostakovich', 'steel',\n",
      "            'database', 'transistor', 'Goethe', 'dog', 'meme', 'spleen',\n",
      "            'morphine', 'maple', \"rubik's cube\", 'souffle', 'chlorine',\n",
      "            'earthworm', 'prune', 'ballet', 'ultrasound', 'bruce lee']\n",
      "\n",
      "corpus = []\n",
      "\n",
      "# search for each article, and if we get a result, store it in the corpus\n",
      "for article in articles:\n",
      "    en = fetch_wiki(article, 'en')\n",
      "    if en:\n",
      "        corpus.append(en)\n",
      "    simple = fetch_wiki(article, 'simple')\n",
      "    if simple:\n",
      "        corpus.append(simple)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert features\n",
      "text, Y = zip(*corpus)\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use SKLearn's train_test_split \n",
      "xtrain, xtest, ytrain, ytest = train_test_split(X, Y)\n",
      "\n",
      "# Create our classifier\n",
      "clf = MultinomialNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n",
      "\n",
      "clf = BernoulliNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 20.00%\n",
        "Accuracy: 60.00%\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# More data\n",
      "\n",
      "That didn't work so well. You can repeat the last cell a few times to see much the accuracy depends on the train/test split. This calls for more data.\n",
      "\n",
      "Use the `.random(n_titles)` method to retrieve a list of 100 more titles and fetch the text for training. Note that `.random` will only return up to 10 titles at a time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wiki.set_lang('simple')\n",
      "\n",
      "for i in xrange(10):\n",
      "    print i  # monitor progress -- this is really slow\n",
      "    for title in wiki.random(10):\n",
      "        # your turn! Find each title in each language in wiki and if you get a result, store it to corpus."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "expected an indented block (<ipython-input-22-865a437c4111>, line 6)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-865a437c4111>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    # your turn! Find each title in each language in wiki and if you get a result, store it to corpus.\u001b[0m\n\u001b[0m                                                                                                      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# we just spent a long time fetching this data, so let's save it so we don't have to do it all over again.\n",
      "import pickle\n",
      "pickle.dump(corpus, open('corpus.p', 'wb'))\n",
      "\n",
      "# restore with: corpus = pickle.load(open('corpus.p', 'rb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# convert features\n",
      "text, Y = zip(*corpus)\n",
      "vectorizer = CountVectorizer()\n",
      "X = vectorizer.fit_transform(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Use SKLearn's train_test_split \n",
      "xtrain, xtest, ytrain, ytest = train_test_split(X, Y)\n",
      "\n",
      "# Create our classifier\n",
      "clf = MultinomialNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))\n",
      "\n",
      "clf = BernoulliNB().fit(xtrain, ytrain)\n",
      "\n",
      "print \"Accuracy: %0.2f%%\" % (100 * clf.score(xtest, ytest))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The variance is a bit contained, but it's still not performing too well. Now it's your turn to try a few things to see if you can improve the performance to something useful. You can try fetching even more data overnight. We'd only fetched the English article summaries, but maybe it'll help to use the full articles. You can try filtering out words < 3 or 4 chars long. We haven't tried allowing bigrams and trigrams yet or removed stop-words. Both the Multinomial and Bernoulli models have model parameters that you can try adjusting. Finally, you might imagine other ways to tweak our model!\n",
      "\n",
      "Here are links to relevant docs:\n",
      "\n",
      "[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
      "[MultinomialNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
      "[BernoulliNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# start here!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}